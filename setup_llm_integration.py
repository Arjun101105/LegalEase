#!/usr/bin/env python3
"""
LLM Integration Setup for LegalEase
Helps configure API keys and test LLM providers
"""

import os
from pathlib import Path
import json
import requests

def print_banner():
    """Print setup banner"""
    print("üöÄ LegalEase LLM Integration Setup")
    print("=" * 50)
    print("This will help you set up LLM providers for enhanced legal text simplification")
    print()

def setup_api_keys():
    """Interactive API key setup"""
    print("üîë API Key Configuration")
    print("-" * 30)
    
    # Create .env file for API keys
    env_file = Path(__file__).parent / ".env"
    env_vars = {}
    
    # Read existing .env if it exists
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                if '=' in line and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    env_vars[key] = value
    
    providers = [
        {
            "name": "Groq",
            "key": "GROQ_API_KEY",
            "description": "Fast Llama models, very affordable (~$0.27/1M tokens)",
            "signup": "https://console.groq.com/",
            "recommended": True
        },
        {
            "name": "OpenAI",
            "key": "OPENAI_API_KEY", 
            "description": "GPT models, good quality but more expensive",
            "signup": "https://platform.openai.com/api-keys",
            "recommended": False
        },
        {
            "name": "Anthropic Claude",
            "key": "ANTHROPIC_API_KEY",
            "description": "Claude models, good for complex legal reasoning",
            "signup": "https://console.anthropic.com/",
            "recommended": False
        }
    ]
    
    print("Available LLM Providers:")
    print()
    
    for provider in providers:
        status = "‚úÖ RECOMMENDED" if provider["recommended"] else "‚ö™ Optional"
        current = "‚úì Configured" if env_vars.get(provider["key"]) else "‚úó Not configured"
        
        print(f"{status} {provider['name']} ({current})")
        print(f"   Description: {provider['description']}")
        print(f"   Sign up: {provider['signup']}")
        print()
        
        # Ask if user wants to configure this provider
        if input(f"Configure {provider['name']}? (y/n): ").lower().startswith('y'):
            api_key = input(f"Enter your {provider['name']} API key: ").strip()
            if api_key:
                env_vars[provider["key"]] = api_key
                print(f"‚úÖ {provider['name']} API key saved")
            else:
                print(f"‚ö†Ô∏è  Skipping {provider['name']}")
        print()
    
    # Save to .env file
    with open(env_file, 'w') as f:
        f.write("# LegalEase LLM API Keys\n")
        f.write("# Generated by setup_llm_integration.py\n\n")
        for key, value in env_vars.items():
            f.write(f"{key}={value}\n")
    
    print(f"üíæ API keys saved to {env_file}")
    print("‚ö†Ô∏è  Keep this file secure and don't share it!")
    
    return env_vars

def test_llm_providers(env_vars):
    """Test configured LLM providers"""
    print("\nüß™ Testing LLM Providers")
    print("-" * 30)
    
    # Load environment variables
    for key, value in env_vars.items():
        os.environ[key] = value
    
    # Import and test
    try:
        from src.llm_integration import LLMSimplifier
        llm = LLMSimplifier()
        
        test_text = "The plaintiff filed a writ petition under Article 32 seeking mandamus."
        
        print("Testing with sample legal text:")
        print(f"'{test_text}'")
        print()
        
        # Test each provider
        providers = [
            ("Groq", llm.simplify_with_groq),
            ("OpenAI", llm.simplify_with_openai),
            ("Local Ollama", llm.simplify_with_local_llm)
        ]
        
        for name, test_func in providers:
            print(f"Testing {name}...")
            try:
                result = test_func(test_text)
                if result:
                    print(f"‚úÖ {name} working! Sample output:")
                    print(f"   {result[:100]}...")
                else:
                    print(f"‚ö†Ô∏è  {name} not configured or not responding")
            except Exception as e:
                print(f"‚ùå {name} error: {e}")
            print()
            
    except ImportError as e:
        print(f"‚ùå Error importing LLM module: {e}")

def setup_local_llm():
    """Help set up local LLM with Ollama"""
    print("üè† Local LLM Setup (Optional)")
    print("-" * 30)
    
    print("Local LLMs run on your computer - completely private but require more resources.")
    print()
    
    if input("Set up local LLM with Ollama? (y/n): ").lower().startswith('y'):
        print("\nüì¶ Ollama Installation:")
        print("1. Download Ollama from: https://ollama.ai/")
        print("2. Install Ollama on your system")
        print("3. Open terminal/command prompt")
        print("4. Run: ollama pull llama2:7b")
        print("5. Wait for download (~3.8GB)")
        print("6. Test: ollama run llama2:7b")
        print()
        print("üí° Alternative models:")
        print("   - ollama pull mistral:7b (faster)")
        print("   - ollama pull codellama:7b (code-focused)")
        print("   - ollama pull llama2:13b (better quality, slower)")
        print()
        
        if input("Test Ollama connection? (y/n): ").lower().startswith('y'):
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    print(f"‚úÖ Ollama is running with {len(models)} models:")
                    for model in models:
                        print(f"   - {model['name']}")
                else:
                    print("‚ö†Ô∏è  Ollama not responding. Make sure it's running.")
            except:
                print("‚ùå Cannot connect to Ollama. Install and start it first.")

def create_demo_script():
    """Create a demo script to test LLM integration"""
    demo_script = '''#!/usr/bin/env python3
"""
Demo script to test LLM-enhanced legal text simplification
"""

import os
from pathlib import Path

# Load environment variables from .env file
env_file = Path(__file__).parent / ".env"
if env_file.exists():
    with open(env_file, 'r') as f:
        for line in f:
            if '=' in line and not line.startswith('#'):
                key, value = line.strip().split('=', 1)
                os.environ[key] = value

from src.llm_integration import LLMSimplifier

def main():
    print("üß™ LegalEase LLM Enhancement Demo")
    print("=" * 40)
    
    # Sample legal texts
    samples = [
        "The plaintiff filed a writ petition under Article 32 of the Constitution seeking mandamus against the respondent for non-compliance with statutory obligations.",
        "The appellant was constrained to file this appeal challenging the impugned order passed by the learned Single Judge.",
        "The party of the first part hereby covenants and agrees to indemnify and hold harmless the party of the second part."
    ]
    
    llm = LLMSimplifier()
    
    for i, sample in enumerate(samples, 1):
        print(f"\\nüìù Sample {i}:")
        print(f"Original: {sample}")
        print()
        
        # Get current system output (simulated)
        current_output = f"In simple terms: {sample.lower()}"
        
        # Try LLM enhancement
        enhanced = llm.enhance_simplification(sample, current_output)
        
        print("Enhanced:", enhanced)
        print("-" * 60)

if __name__ == "__main__":
    main()
'''
    
    demo_file = Path(__file__).parent / "demo_llm.py"
    with open(demo_file, 'w') as f:
        f.write(demo_script)
    
    print(f"üìù Demo script created: {demo_file}")
    print("Run it with: python demo_llm.py")

def main():
    """Main setup function"""
    print_banner()
    
    # Step 1: Configure API keys
    env_vars = setup_api_keys()
    
    # Step 2: Test providers
    if env_vars:
        test_llm_providers(env_vars)
    
    # Step 3: Local LLM setup
    setup_local_llm()
    
    # Step 4: Create demo script
    create_demo_script()
    
    print("\nüéâ LLM Integration Setup Complete!")
    print("-" * 40)
    print("Next steps:")
    print("1. Test with: python demo_llm.py")
    print("2. Run enhanced CLI: python src/cli_app.py")
    print("3. For batch processing: python src/cli_app.py --input file.txt")
    print()
    print("üí° The system will fallback to your current FLAN-T5 model if LLMs fail.")

if __name__ == "__main__":
    main()